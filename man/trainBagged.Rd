% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainTfModel.R
\name{trainBagged}
\alias{trainBagged}
\title{Training transcription factor-specific tree-based gradient boosting Models}
\usage{
trainBagged(
  tfName,
  featMat,
  measureName = c("classif.aucpr", "classif.logloss"),
  evalRounds = 100,
  earlyStoppingRounds = 100,
  posFrac = 0.25,
  loContext = FALSE,
  tuneHyperparams = TRUE,
  seed = 42,
  numThreads = 10,
  BPPARAM = SerialParam
)
}
\arguments{
\item{tfName}{Name of transcription factor to train model for.}

\item{featMat}{Feature matrix as constructed by \link{getFeatureMatrix}.}

\item{measureName}{Measure used for hyperparameter selection.
Either area under the precision-recall curve computed using \link[PRROC:pr.curve]{PRROC::pr.curve} ("classif.aucpr") or
logloss as implemented by \link[mlr3measures:logloss]{mlr3measures::logloss}.}

\item{evalRounds}{Number of evaluation rounds for the hyperparameter selection rounds.}

\item{earlyStoppingRounds}{Number of early stopping rounds for the hyperparameter selection and training of the \link[lightgbm:lightgbm]{lightgbm::lightgbm} model.}

\item{posFrac}{Fraction of positives to use for the training of the model.
Negatives will be supsampled to achieve the specified fraction.}

\item{loContext}{Should cellular-contexts be used for leave-one-context-out rounds during hyperparameter selection.
Only works if more than one cellular-context is contained within the feature matrix.}

\item{tuneHyperparams}{If hyperparameters should be tuned with \link[mlr3mbo:mlr_tuners_mbo]{mlr3mbo::TunerMbo}. Recommend to have this turned on (\code{tuneHyperparams=TRUE}).
Otherwise (hopefully) sensible defaults are used.}

\item{seed}{Integer value for setting the seed for random number generation with \link[base:Random]{base::set.seed}.}

\item{numThreads}{Total number of threads to be used. In case \link[BiocParallel:MulticoreParam-class]{BiocParallel::MulticoreParam} or \link[BiocParallel:SnowParam-class]{BiocParallel::SnowParam} with several workers are
are specified as parallel back-ends, \code{floor(numThreads/nWorker)} threads are used per worker.}

\item{BPPARAM}{Parallel back-end to be used. Passed to \code{\link[BiocParallel:bpmapply]{BiocParallel::bpmapply()}}.}
}
\value{
A list of four \link[lightgbm:lightgbm]{lightgbm::lightgbm} models trained on different strata of the data.
}
\description{
Trains a bag of four tree-based gradient boosting models of the \link[lightgbm:lightgbm]{lightgbm::lightgbm} library.
Hyperparameter selection is performed for each model seperately using model-based optimization by deploying the \link{mlr3tuning} library.
The lightgbm classification learner used for the hyperparameter selection has been copied from the GitHub repository \url{https://github.com/mlr-org/mlr3extralearners}, whichs
contains the remote package \link{mlr3extralearners} developed by Raphael Sonabend and Patrick Schratz and Sebastian Fischer.
}
